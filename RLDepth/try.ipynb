{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c0b82b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching reference HEAD\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AI2-THOR WARNING] There has been an update to ProcTHOR-10K that must be used with AI2-THOR version 5.0+. To use the new version of ProcTHOR-10K, please update AI2-THOR to version 5.0+ by running:\n",
      "    pip install --upgrade ai2thor\n",
      "Alternatively, to downgrade to the old version of ProcTHOR-10K, run:\n",
      "   prior.load_dataset(\"procthor-10k\", revision=\"ab3cacd0fc17754d4c080a3fd50b18395fae8647\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 23355.71it/s]\n",
      "Loading val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 23646.80it/s]\n",
      "Loading test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 24739.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict(\n",
       "    train=Dataset(\n",
       "    dataset=procthor-dataset,\n",
       "    size=10000,\n",
       "    split=train\n",
       "),\n",
       "    val=Dataset(\n",
       "    dataset=procthor-dataset,\n",
       "    size=1000,\n",
       "    split=val\n",
       "),\n",
       "    test=Dataset(\n",
       "    dataset=procthor-dataset,\n",
       "    size=1000,\n",
       "    split=test\n",
       ")\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import prior\n",
    "\n",
    "dataset = prior.load_dataset(\"procthor-10k\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a011a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def teleport(controller, target=None):\n",
    "    event = controller.step(\"GetReachablePositions\")\n",
    "    reachable_positions = event.metadata[\"actionReturn\"]\n",
    "    # Pick a random target\n",
    "    if target is None:\n",
    "        target = np.random.choice(reachable_positions)\n",
    "\n",
    "    event = controller.step(\n",
    "        action=\"TeleportFull\",\n",
    "        x=target[\"x\"],\n",
    "        y=target[\"y\"],\n",
    "        z=target[\"z\"],\n",
    "        rotation={\"x\": 0, \"y\": 0, \"z\": 0},\n",
    "        horizon=0,\n",
    "        standing=True\n",
    "    )\n",
    "\n",
    "    return event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d347af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl import PPO, ActorCritic, Env, RolloutBuffer, ClipEnvNoPenalty, CLIPNovelty\n",
    "from models import FrozenResNetEncoder, SlidingWindowTransformerActor, ConvDepthEncoder, SlidingWindowTransformerCritic\n",
    "from cons import MINIBATCHES, EPISODE_STEPS, FEAT_DIM, NUM_ACTIONS, DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7133071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from rl import save_actor_critic\n",
    "import numpy as np\n",
    "from cons import NUM_ACTIONS, EPISODE_STEPS, MINIBATCHES, DEVICE\n",
    "\n",
    "def train(controller, name: str, ppo: PPO, env: Env, actor_critic: ActorCritic, total_updates=10):\n",
    "\n",
    "    event = controller.step(\"Pass\")\n",
    "    rewards = []\n",
    "    episode_rewards = []\n",
    "\n",
    "    for upd in range(total_updates):\n",
    "        buf = RolloutBuffer()\n",
    "\n",
    "        for mb in range(MINIBATCHES):\n",
    "\n",
    "            episode_seq = []\n",
    "            episode_reward = 0.0\n",
    "            actions_seq = []\n",
    "\n",
    "            for t in range(1, EPISODE_STEPS + 1):\n",
    "\n",
    "                # =============================\n",
    "                # RAW RGB\n",
    "                # =============================\n",
    "                rgb_frame = ppo.obs_from_event(event.frame)  # (3,H,W)\n",
    "\n",
    "                # =============================\n",
    "                # RAW DEPTH  (normalized)\n",
    "                # =============================\n",
    "                depth_tensor = ppo.obs_from_event(event.depth_frame) / 10.\n",
    "\n",
    "                # =============================\n",
    "                # Encode for policy (no grad)\n",
    "                # =============================\n",
    "                with torch.no_grad():\n",
    "                    rgb_input = rgb_frame.unsqueeze(0).unsqueeze(0)     # (1,1,3,H,W)\n",
    "                    depth_input = depth_tensor.unsqueeze(0).unsqueeze(0)  # (1,1,1,H,W)\n",
    "\n",
    "                    rgb_embed = actor_critic.rgb_encoder(rgb_input).squeeze(0).squeeze(0)\n",
    "                    depth_embed = actor_critic.depth_encoder(depth_input).squeeze(0).squeeze(0)\n",
    "\n",
    "                    fused_embed = torch.cat([rgb_embed, depth_embed], dim=-1)\n",
    "\n",
    "                # Build sequence for transformer\n",
    "                episode_seq.append(fused_embed)\n",
    "                obs_seq = torch.stack(episode_seq, dim=0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "                # =============================\n",
    "                # Action history\n",
    "                # =============================\n",
    "                if len(actions_seq) == 0:\n",
    "                    actions_seq.append(torch.randint(0, NUM_ACTIONS, ()).item())\n",
    "\n",
    "                actions_tensor = torch.tensor(actions_seq, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
    "\n",
    "                # =============================\n",
    "                # Policy + Value\n",
    "                # =============================\n",
    "                logits, value = ppo.act_and_value(obs_seq, actions_tensor, actor_critic)\n",
    "                dist = torch.distributions.Categorical(logits=logits)\n",
    "                action_idx = dist.sample().item()\n",
    "                logp = dist.log_prob(torch.tensor(action_idx, device=DEVICE)).item()\n",
    "\n",
    "                # =============================\n",
    "                # Step environment\n",
    "                # =============================\n",
    "                event, reward = env.step_env(controller, action_idx)\n",
    "                done = (t == EPISODE_STEPS)\n",
    "\n",
    "                # =============================\n",
    "                # Store RAW frames ONLY\n",
    "                # =============================\n",
    "                buf.add(\n",
    "                    rgb_frame=rgb_frame.cpu(),\n",
    "                    depth_frame=depth_tensor.cpu(),\n",
    "                    action_idx=action_idx,\n",
    "                    logp=logp,\n",
    "                    reward=reward,\n",
    "                    value=value,\n",
    "                    done=done\n",
    "                )\n",
    "\n",
    "                rewards.append(reward)\n",
    "                episode_reward += reward / EPISODE_STEPS\n",
    "                actions_seq.append(action_idx)\n",
    "\n",
    "                # =============================\n",
    "                # Reset episode if done\n",
    "                # =============================\n",
    "                if done:\n",
    "                    env.reset()\n",
    "                    if np.random.rand() > 0.5:\n",
    "                        event = teleport(controller)\n",
    "\n",
    "            episode_rewards.append(episode_reward)\n",
    "\n",
    "        # =============================\n",
    "        # PPO UPDATE (re-encodes raw)\n",
    "        # =============================\n",
    "        ppo.ppo_update(buf, actor_critic)\n",
    "        save_actor_critic(actor_critic, name)\n",
    "\n",
    "        print(f\"Update {upd+1}/{total_updates} â€” steps: {len(buf)}\")\n",
    "\n",
    "        torch.cuda.empty_cache()  # optional safety\n",
    "\n",
    "    return buf, rewards, episode_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efc1948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTROPY_COEF = 0.05\n",
    "\n",
    "ppo = PPO(ENTROPY_COEF)\n",
    "encoder = FrozenResNetEncoder()\n",
    "depth_encoder = ConvDepthEncoder()\n",
    "actor = SlidingWindowTransformerActor(FEAT_DIM, NUM_ACTIONS)\n",
    "critic = SlidingWindowTransformerCritic(FEAT_DIM)\n",
    "clip_novelty = CLIPNovelty()\n",
    "clip_env = ClipEnvNoPenalty(clip_novelty)\n",
    "clip_actor_critic = ActorCritic(encoder, depth_encoder, actor, critic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7e111fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ðŸ”] Actor-Critic checkpoint loaded from data/weight_long.pt\n"
     ]
    }
   ],
   "source": [
    "from rl import load_actor_critic\n",
    "\n",
    "load_actor_critic(clip_actor_critic, \"data/weight_long.pt\", device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3da78fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m controller \u001b[38;5;241m=\u001b[39m Controller(scene\u001b[38;5;241m=\u001b[39mhouse, snapToGrid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rotateStepDegrees\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, renderDepthImage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m teleport(controller)\n\u001b[0;32m---> 10\u001b[0m buf, rewards, episode_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontroller\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mppo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_actor_critic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(rewards)\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[4], line 102\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(controller, name, ppo, env, actor_critic, total_updates)\u001b[0m\n\u001b[1;32m     97\u001b[0m     episode_rewards\u001b[38;5;241m.\u001b[39mappend(episode_reward)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# =============================\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# PPO UPDATE (re-encodes raw)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# =============================\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mppo_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_critic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m save_actor_critic(actor_critic, name)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mupd\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_updates\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m â€” steps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(buf)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/5.UofT/Courses/CSC2503H-ComputerVision/Project/NavAssistant/RLDepth/rl.py:447\u001b[0m, in \u001b[0;36mPPO.ppo_update\u001b[0;34m(self, buffer, actor_critic)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# 4.4 optimize\u001b[39;00m\n\u001b[1;32m    446\u001b[0m actor_critic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 447\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28mlist\u001b[39m(actor_critic\u001b[38;5;241m.\u001b[39mrgb_encoder\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28mlist\u001b[39m(actor_critic\u001b[38;5;241m.\u001b[39mdepth_encoder\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    453\u001b[0m     MAX_GRAD_NORM\n\u001b[1;32m    454\u001b[0m )\n\u001b[1;32m    455\u001b[0m actor_critic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/5.UofT/Courses/CSC2503H-ComputerVision/Project/NavAssistant/.venv/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/5.UofT/Courses/CSC2503H-ComputerVision/Project/NavAssistant/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/5.UofT/Courses/CSC2503H-ComputerVision/Project/NavAssistant/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ai2thor.controller import Controller\n",
    "\n",
    "NUM_ENV = 50\n",
    "for i in range(NUM_ENV):\n",
    "    try:\n",
    "        rand_env = torch.randint(0, len(dataset[\"train\"]), (1,)).item()\n",
    "        house = dataset[\"train\"][rand_env]\n",
    "        controller = Controller(scene=house, snapToGrid=False, rotateStepDegrees=30, renderDepthImage=True)\n",
    "        teleport(controller)\n",
    "        buf, rewards, episode_rewards = train(controller, \"weight.pt\", ppo, clip_env, clip_actor_critic, 5)\n",
    "        plt.plot(rewards)\n",
    "        plt.show()\n",
    "        plt.plot(episode_rewards)\n",
    "        plt.show()\n",
    "    finally:\n",
    "        controller.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "00b8065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (300, 300) to (304, 304) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ðŸŽžï¸] Saved video to rollout.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1.0985212326049805, 1.6973028182983398],\n",
       " [1.0985212326049805, 1.9473028182983398],\n",
       " [1.0985212326049805, 2.19730281829834],\n",
       " [1.0985212326049805, 2.44730281829834],\n",
       " [1.0985212326049805, 2.44730281829834],\n",
       " [0.9735212326049805, 2.663809061050415],\n",
       " [0.9735212326049805, 2.663809061050415],\n",
       " [0.7570148706436157, 2.788809061050415],\n",
       " [0.7570148706436157, 2.788809061050415],\n",
       " [0.7570148706436157, 2.788809061050415],\n",
       " [0.7570148706436157, 2.788809061050415],\n",
       " [0.7570148706436157, 2.788809061050415],\n",
       " [0.7570148706436157, 2.788809061050415],\n",
       " [0.7570148706436157, 2.788809061050415],\n",
       " [0.7570148706436157, 2.788809061050415],\n",
       " [0.7570148706436157, 2.788809061050415],\n",
       " [0.7570148706436157, 2.788809061050415],\n",
       " [0.7570148706436157, 3.038809061050415],\n",
       " [0.7570148706436157, 3.038809061050415],\n",
       " [0.8820148706436157, 3.2553153038024902],\n",
       " [0.8820148706436157, 3.2553153038024902],\n",
       " [1.0985212326049805, 3.3803153038024902],\n",
       " [1.0985212326049805, 3.3803153038024902],\n",
       " [1.0985212326049805, 3.3803153038024902],\n",
       " [1.0985212326049805, 3.6303153038024902],\n",
       " [1.0985212326049805, 3.8803153038024902],\n",
       " [1.0985212326049805, 3.8803153038024902],\n",
       " [1.0985212326049805, 3.8803153038024902],\n",
       " [1.0985212326049805, 4.13031530380249],\n",
       " [1.0985212326049805, 4.38031530380249],\n",
       " [1.0985212326049805, 4.38031530380249],\n",
       " [1.0985212326049805, 4.38031530380249],\n",
       " [1.3150275945663452, 4.50531530380249],\n",
       " [1.53153395652771, 4.63031530380249],\n",
       " [1.7480403184890747, 4.75531530380249],\n",
       " [1.7480403184890747, 4.75531530380249],\n",
       " [1.9980403184890747, 4.75531530380249],\n",
       " [2.248040199279785, 4.75531530380249],\n",
       " [2.498040199279785, 4.75531530380249],\n",
       " [2.748040199279785, 4.75531530380249],\n",
       " [2.998040199279785, 4.75531530380249],\n",
       " [3.248040199279785, 4.75531530380249],\n",
       " [3.248040199279785, 4.75531530380249],\n",
       " [3.4645464420318604, 4.63031530380249],\n",
       " [3.6810526847839355, 4.50531530380249],\n",
       " [3.6810526847839355, 4.50531530380249],\n",
       " [3.9310526847839355, 4.50531530380249],\n",
       " [4.1810526847839355, 4.50531530380249],\n",
       " [4.4310526847839355, 4.50531530380249],\n",
       " [4.6810526847839355, 4.50531530380249],\n",
       " [4.9310526847839355, 4.50531530380249],\n",
       " [5.1810526847839355, 4.50531530380249],\n",
       " [5.4310526847839355, 4.50531530380249],\n",
       " [5.6810526847839355, 4.50531530380249],\n",
       " [5.9310526847839355, 4.50531530380249],\n",
       " [6.1810526847839355, 4.50531530380249],\n",
       " [6.4310526847839355, 4.50531530380249],\n",
       " [6.6810526847839355, 4.50531530380249],\n",
       " [6.9310526847839355, 4.50531530380249],\n",
       " [7.1810526847839355, 4.50531530380249],\n",
       " [7.4310526847839355, 4.50531530380249],\n",
       " [7.6810526847839355, 4.50531530380249],\n",
       " [7.6810526847839355, 4.50531530380249],\n",
       " [7.89755916595459, 4.38031530380249],\n",
       " [8.114065170288086, 4.25531530380249],\n",
       " [8.114065170288086, 4.25531530380249],\n",
       " [8.114065170288086, 4.25531530380249],\n",
       " [8.114065170288086, 4.25531530380249],\n",
       " [8.114065170288086, 4.25531530380249],\n",
       " [8.114065170288086, 4.25531530380249],\n",
       " [8.114065170288086, 4.25531530380249],\n",
       " [8.114065170288086, 4.25531530380249],\n",
       " [8.114065170288086, 4.25531530380249],\n",
       " [8.114065170288086, 4.25531530380249],\n",
       " [8.114065170288086, 4.25531530380249],\n",
       " [8.114065170288086, 4.25531530380249],\n",
       " [8.114065170288086, 4.25531530380249],\n",
       " [7.864065170288086, 4.25531530380249],\n",
       " [7.614065170288086, 4.25531530380249],\n",
       " [7.364065170288086, 4.25531530380249],\n",
       " [7.114065170288086, 4.25531530380249],\n",
       " [6.864065170288086, 4.25531530380249],\n",
       " [6.614065170288086, 4.25531530380249],\n",
       " [6.364065170288086, 4.25531530380249],\n",
       " [6.114065170288086, 4.25531530380249],\n",
       " [5.864065170288086, 4.25531530380249],\n",
       " [5.614065170288086, 4.25531530380249],\n",
       " [5.364065170288086, 4.25531530380249],\n",
       " [5.114065170288086, 4.25531530380249],\n",
       " [4.864065170288086, 4.25531530380249],\n",
       " [4.614065170288086, 4.25531530380249],\n",
       " [4.364065170288086, 4.25531530380249],\n",
       " [4.114065170288086, 4.25531530380249],\n",
       " [3.864065170288086, 4.25531530380249],\n",
       " [3.614065170288086, 4.25531530380249],\n",
       " [3.364065170288086, 4.25531530380249],\n",
       " [3.114065170288086, 4.25531530380249],\n",
       " [2.864065170288086, 4.25531530380249],\n",
       " [2.614065170288086, 4.25531530380249],\n",
       " [2.614065170288086, 4.25531530380249],\n",
       " [2.614065170288086, 4.25531530380249],\n",
       " [2.614065170288086, 4.25531530380249],\n",
       " [2.614065170288086, 4.25531530380249],\n",
       " [2.364065170288086, 4.25531530380249],\n",
       " [2.114065170288086, 4.25531530380249],\n",
       " [2.114065170288086, 4.25531530380249],\n",
       " [2.114065170288086, 4.25531530380249],\n",
       " [2.114065170288086, 4.25531530380249],\n",
       " [2.114065170288086, 4.25531530380249],\n",
       " [2.114065170288086, 4.25531530380249],\n",
       " [2.114065170288086, 4.25531530380249],\n",
       " [2.114065170288086, 4.25531530380249],\n",
       " [2.114065170288086, 4.25531530380249],\n",
       " [2.114065170288086, 4.25531530380249],\n",
       " [2.3305716514587402, 4.38031530380249],\n",
       " [2.5470781326293945, 4.50531530380249],\n",
       " [2.5470781326293945, 4.50531530380249],\n",
       " [2.5470781326293945, 4.50531530380249],\n",
       " [2.5470781326293945, 4.50531530380249],\n",
       " [2.5470781326293945, 4.50531530380249],\n",
       " [2.5470781326293945, 4.50531530380249],\n",
       " [2.5470781326293945, 4.50531530380249],\n",
       " [2.5470781326293945, 4.50531530380249],\n",
       " [2.5470781326293945, 4.50531530380249],\n",
       " [2.5470781326293945, 4.50531530380249],\n",
       " [2.5470781326293945, 4.50531530380249],\n",
       " [2.5470781326293945, 4.50531530380249],\n",
       " [2.5470781326293945, 4.50531530380249],\n",
       " [2.5470781326293945, 4.50531530380249],\n",
       " [2.5470781326293945, 4.50531530380249],\n",
       " [2.5470781326293945, 4.50531530380249],\n",
       " [2.7970781326293945, 4.50531530380249],\n",
       " [3.0470781326293945, 4.50531530380249],\n",
       " [3.2970781326293945, 4.50531530380249],\n",
       " [3.5470781326293945, 4.50531530380249],\n",
       " [3.7970781326293945, 4.50531530380249],\n",
       " [4.0470781326293945, 4.50531530380249],\n",
       " [4.2970781326293945, 4.50531530380249],\n",
       " [4.5470781326293945, 4.50531530380249],\n",
       " [4.7970781326293945, 4.50531530380249],\n",
       " [5.0470781326293945, 4.50531530380249],\n",
       " [5.2970781326293945, 4.50531530380249],\n",
       " [5.5470781326293945, 4.50531530380249],\n",
       " [5.7970781326293945, 4.50531530380249],\n",
       " [6.0470781326293945, 4.50531530380249],\n",
       " [6.2970781326293945, 4.50531530380249],\n",
       " [6.5470781326293945, 4.50531530380249],\n",
       " [6.7970781326293945, 4.50531530380249],\n",
       " [7.0470781326293945, 4.50531530380249],\n",
       " [7.2970781326293945, 4.50531530380249],\n",
       " [7.5470781326293945, 4.50531530380249],\n",
       " [7.7970781326293945, 4.50531530380249],\n",
       " [7.7970781326293945, 4.50531530380249],\n",
       " [8.01358413696289, 4.38031530380249],\n",
       " [8.01358413696289, 4.38031530380249],\n",
       " [8.13858413696289, 4.163808822631836],\n",
       " [8.13858413696289, 4.163808822631836],\n",
       " [8.13858413696289, 4.163808822631836],\n",
       " [8.13858413696289, 4.163808822631836],\n",
       " [8.13858413696289, 4.163808822631836],\n",
       " [7.888584136962891, 4.163808822631836],\n",
       " [7.888584136962891, 4.163808822631836],\n",
       " [7.888584136962891, 4.163808822631836],\n",
       " [7.888584136962891, 4.163808822631836],\n",
       " [7.888584136962891, 4.163808822631836],\n",
       " [7.638584136962891, 4.163808822631836],\n",
       " [7.638584136962891, 4.163808822631836],\n",
       " [7.638584136962891, 4.163808822631836],\n",
       " [7.388584136962891, 4.163808822631836],\n",
       " [7.138584136962891, 4.163808822631836],\n",
       " [6.888584136962891, 4.163808822631836],\n",
       " [6.638584136962891, 4.163808822631836],\n",
       " [6.388584136962891, 4.163808822631836],\n",
       " [6.138584136962891, 4.163808822631836],\n",
       " [5.888584136962891, 4.163808822631836],\n",
       " [5.638584136962891, 4.163808822631836],\n",
       " [5.388584136962891, 4.163808822631836],\n",
       " [5.138584136962891, 4.163808822631836],\n",
       " [4.888584136962891, 4.163808822631836],\n",
       " [4.638584136962891, 4.163808822631836],\n",
       " [4.388584136962891, 4.163808822631836],\n",
       " [4.138584136962891, 4.163808822631836],\n",
       " [3.8885841369628906, 4.163808822631836],\n",
       " [3.6385841369628906, 4.163808822631836],\n",
       " [3.3885841369628906, 4.163808822631836],\n",
       " [3.3885841369628906, 4.163808822631836],\n",
       " [3.3885841369628906, 4.163808822631836],\n",
       " [3.1385841369628906, 4.163808822631836],\n",
       " [2.8885841369628906, 4.163808822631836],\n",
       " [2.6385841369628906, 4.163808822631836],\n",
       " [2.3885841369628906, 4.163808822631836],\n",
       " [2.3885841369628906, 4.163808822631836],\n",
       " [2.1720778942108154, 4.038808822631836],\n",
       " [2.1720778942108154, 4.038808822631836],\n",
       " [1.9220778942108154, 4.038808822631836],\n",
       " [1.6720778942108154, 4.038808822631836],\n",
       " [1.4220778942108154, 4.038808822631836],\n",
       " [1.1720778942108154, 4.038808822631836],\n",
       " [1.1720778942108154, 4.038808822631836],\n",
       " [0.9555715322494507, 4.163808822631836],\n",
       " [0.7390651702880859, 4.288808822631836],\n",
       " [0.5225588083267212, 4.413808822631836],\n",
       " [0.5225588083267212, 4.413808822631836],\n",
       " [0.3975587785243988, 4.63031530380249],\n",
       " [0.3975587785243988, 4.63031530380249],\n",
       " [0.3975587785243988, 4.63031530380249],\n",
       " [0.3975587785243988, 4.63031530380249],\n",
       " [0.3975587785243988, 4.63031530380249],\n",
       " [0.3975587785243988, 4.63031530380249],\n",
       " [0.3975587785243988, 4.63031530380249],\n",
       " [0.6475588083267212, 4.63031530380249],\n",
       " [0.8975588083267212, 4.63031530380249],\n",
       " [1.1475588083267212, 4.63031530380249],\n",
       " [1.3975588083267212, 4.63031530380249],\n",
       " [1.6475588083267212, 4.63031530380249],\n",
       " [1.8975588083267212, 4.63031530380249],\n",
       " [2.1475586891174316, 4.63031530380249],\n",
       " [2.3975586891174316, 4.63031530380249],\n",
       " [2.6475586891174316, 4.63031530380249],\n",
       " [2.8975586891174316, 4.63031530380249],\n",
       " [3.1475586891174316, 4.63031530380249],\n",
       " [3.3975586891174316, 4.63031530380249],\n",
       " [3.6475586891174316, 4.63031530380249],\n",
       " [3.8975586891174316, 4.63031530380249],\n",
       " [4.147558689117432, 4.63031530380249],\n",
       " [4.397558689117432, 4.63031530380249],\n",
       " [4.647558689117432, 4.63031530380249],\n",
       " [4.897558689117432, 4.63031530380249],\n",
       " [5.147558689117432, 4.63031530380249],\n",
       " [5.397558689117432, 4.63031530380249],\n",
       " [5.647558689117432, 4.63031530380249],\n",
       " [5.897558689117432, 4.63031530380249],\n",
       " [6.147558689117432, 4.63031530380249],\n",
       " [6.397558689117432, 4.63031530380249],\n",
       " [6.647558689117432, 4.63031530380249],\n",
       " [6.897558689117432, 4.63031530380249],\n",
       " [7.147558689117432, 4.63031530380249],\n",
       " [7.397558689117432, 4.63031530380249],\n",
       " [7.647558689117432, 4.63031530380249],\n",
       " [7.897558689117432, 4.63031530380249],\n",
       " [7.897558689117432, 4.63031530380249],\n",
       " [8.114065170288086, 4.50531530380249],\n",
       " [8.114065170288086, 4.50531530380249],\n",
       " [8.114065170288086, 4.50531530380249],\n",
       " [8.114065170288086, 4.50531530380249],\n",
       " [8.114065170288086, 4.50531530380249],\n",
       " [8.114065170288086, 4.50531530380249],\n",
       " [8.114065170288086, 4.50531530380249],\n",
       " [8.114065170288086, 4.50531530380249],\n",
       " [8.114065170288086, 4.50531530380249],\n",
       " [7.864065170288086, 4.50531530380249],\n",
       " [7.614065170288086, 4.50531530380249],\n",
       " [7.364065170288086, 4.50531530380249],\n",
       " [7.364065170288086, 4.50531530380249],\n",
       " [7.147558689117432, 4.63031530380249],\n",
       " [7.147558689117432, 4.63031530380249]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cons import ACTIONS\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "import imageio\n",
    "\n",
    "\n",
    "def inference(get_distribution: Callable[[torch.Tensor, ActorCritic], torch.distributions.Categorical], init_position: dict[str, float], env: Env, actor_critic: ActorCritic, plot=True):\n",
    "    n = 256\n",
    "    n_row = 32\n",
    "    positions = []\n",
    "\n",
    "    plt.figure(figsize=(n // n_row * 2, n_row * 2))\n",
    "    event = teleport(controller, init_position)\n",
    "    episode_seq = deque(maxlen=EPISODE_STEPS)\n",
    "    actions_seq = deque(maxlen=EPISODE_STEPS)\n",
    "    raw_obs = []\n",
    "    for t in range(1, n + 1):\n",
    "        positions.append([event.metadata[\"agent\"][\"position\"][\"x\"], event.metadata[\"agent\"][\"position\"][\"z\"]])\n",
    "        with torch.no_grad():\n",
    "            obs_t = ppo.obs_from_event(event)  # (C,H,W)\n",
    "            obs_t_encoded = actor_critic.actor_critic_encoder(obs_t.unsqueeze(0).unsqueeze(0)).squeeze(0).squeeze(0)\n",
    "            obs_seq = torch.stack(list(episode_seq) + [obs_t_encoded], dim=0).unsqueeze(0).to(device=DEVICE)\n",
    "\n",
    "        if len(actions_seq) == 0:\n",
    "            actions_seq.append(torch.randint(0, NUM_ACTIONS, (1, 1)).item())\n",
    "        \n",
    "        actions_tensor = torch.tensor(actions_seq, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
    "        dist = get_distribution(obs_seq, actions_tensor, actor_critic)\n",
    "        action_idx = dist.sample()\n",
    "        logp = dist.log_prob(action_idx)\n",
    "        \n",
    "        action_idx, logp = action_idx.item(), logp.item()\n",
    "        event, reward = env.step_env(controller, action_idx)\n",
    "\n",
    "        # store one step\n",
    "        episode_seq.append(obs_t_encoded)\n",
    "        actions_seq.append(action_idx)\n",
    "        raw_obs.append(obs_t)\n",
    "        \n",
    "        if plot:\n",
    "            # Plot frame and action\n",
    "            plt.subplot(n_row, n // n_row, t)\n",
    "            plt.title(\"action: \" + ACTIONS[action_idx] + \", r: \" + f\"{reward:.2f}\" + \"\\n, prob = \" + f\"{torch.exp(dist.log_prob(torch.tensor([0, 1, 2], device=DEVICE))).cpu().numpy()}\", fontsize=5)\n",
    "            plt.axis(False)\n",
    "            plt.imshow(event.frame)\n",
    "    if plot:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # ---- Plot 2D trajectory of the agent ----\n",
    "        positions = np.array(positions)\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.plot(positions[:, 0], positions[:, 1], \"-o\", markersize=3)\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"z\")\n",
    "        plt.title(\"Agent trajectory over n steps\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    return raw_obs\n",
    "\n",
    "def inference_video_mp4(\n",
    "    get_distribution,\n",
    "    init_position: dict[str, float],\n",
    "    env: Env,\n",
    "    actor_critic: ActorCritic,\n",
    "    video_path=\"rollout.mp4\",\n",
    "    fps=10,\n",
    "    n_steps=256\n",
    "):\n",
    "    episode_seq = deque(maxlen=EPISODE_STEPS)\n",
    "    actions_seq = deque(maxlen=EPISODE_STEPS)\n",
    "\n",
    "    writer = imageio.get_writer(video_path, fps=fps)\n",
    "\n",
    "    # start episode\n",
    "    event = teleport(controller, init_position)\n",
    "    positions = []\n",
    "\n",
    "    for t in range(1, n_steps + 1):\n",
    "\n",
    "        # ---- Add current frame to video ----\n",
    "        writer.append_data(event.frame[:, :, ::-1])   # convert RGBâ†’BGR if needed\n",
    "\n",
    "        # track positions\n",
    "        positions.append([\n",
    "            event.metadata[\"agent\"][\"position\"][\"x\"],\n",
    "            event.metadata[\"agent\"][\"position\"][\"z\"],\n",
    "        ])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            obs_t = ppo.obs_from_event(event)  \n",
    "            obs_enc = actor_critic.actor_critic_encoder(\n",
    "                obs_t.unsqueeze(0).unsqueeze(0)\n",
    "            ).squeeze(0).squeeze(0)\n",
    "\n",
    "            obs_seq = torch.stack(\n",
    "                list(episode_seq) + [obs_enc], dim=0\n",
    "            ).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        # ---- Action ----\n",
    "        if len(actions_seq) == 0:\n",
    "            actions_seq.append(torch.randint(0, NUM_ACTIONS, ()).item())\n",
    "\n",
    "        actions_tensor = torch.tensor(\n",
    "            actions_seq, dtype=torch.long, device=DEVICE\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        dist = get_distribution(obs_seq, actions_tensor, actor_critic)\n",
    "        action_idx = dist.sample().item()\n",
    "\n",
    "        # ---- Step env ----\n",
    "        event, reward = env.step_env(controller, action_idx)\n",
    "\n",
    "        # ---- Store ----\n",
    "        episode_seq.append(obs_enc)\n",
    "        actions_seq.append(action_idx)\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"[ðŸŽžï¸] Saved video to {video_path}\")\n",
    "\n",
    "    return positions\n",
    "\n",
    "\n",
    "def get_distributions(obs_seq, actions_tensor, clip_actor_critic):\n",
    "    logits, value = ppo.act_and_value(obs_seq, actions_tensor, clip_actor_critic)\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "    return dist\n",
    "\n",
    "event = teleport(controller)\n",
    "init_pos = event.metadata[\"agent\"][\"position\"]\n",
    "inference_video_mp4(get_distributions, init_pos, clip_env, clip_actor_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23ba3d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in /home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages (2.37.0)\n",
      "Requirement already satisfied: imageio-ffmpeg in /home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy in /home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages (from imageio) (2.0.2)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages (from imageio) (11.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imageio imageio-ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a46bc902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae83af17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: 0.877001953125\n",
      "Rand: 0.86337890625\n",
      "Policy: 0.87021484375\n",
      "Rand: 0.8068359375\n",
      "Policy: 0.83388671875\n",
      "Rand: 0.845361328125\n",
      "Policy: 0.8791015625\n",
      "Rand: 0.876318359375\n",
      "Policy: 0.80693359375\n",
      "Rand: 0.79033203125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def preprocess_clip(x):\n",
    "    # Resize to CLIP resolution\n",
    "    x = F.interpolate(x, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "    # Normalize (CLIP standard mean/std)\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=x.device).view(1, 3, 1, 1)\n",
    "    std  = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=x.device).view(1, 3, 1, 1)\n",
    "    return (x - mean) / std\n",
    "\n",
    "def get_average(input_img):\n",
    "    # ---- 1. Sample 50 random images ----\n",
    "    n = min(50, len(input_img))\n",
    "    indices = random.sample(range(len(input_img)), n)\n",
    "    imgs = torch.stack([input_img[i] for i in indices]).to(DEVICE)  # (50, 3, H, W)\n",
    "\n",
    "    # ---- 2. Preprocess for CLIP ----\n",
    "    imgs_clip = preprocess_clip(imgs).half()  # normalized, resized\n",
    "\n",
    "    # ---- 3. Encode with CLIP visual encoder ----\n",
    "    with torch.no_grad():\n",
    "        embeds = model.visual(imgs_clip)       # (50, D)\n",
    "        embeds = embeds / embeds.norm(dim=-1, keepdim=True)  # normalize to unit sphere\n",
    "\n",
    "    # ---- 4. Compute cosine similarity ----\n",
    "    sim_matrix = embeds @ embeds.T             # (50, 50)\n",
    "    # Optional: remove self-similarity (diagonal = 1)\n",
    "    sim_matrix.fill_diagonal_(0)\n",
    "\n",
    "    # ---- 5. Summary statistics ----\n",
    "    mean_sim = sim_matrix.mean().item()\n",
    "\n",
    "    return mean_sim\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    event = teleport(controller)\n",
    "    init_pos = event.metadata[\"agent\"][\"position\"]\n",
    "    \n",
    "    def get_distributions(obs_seq, actions_tensor, clip_actor_critic):\n",
    "        logits, value = ppo.act_and_value(obs_seq, actions_tensor, clip_actor_critic)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        return dist\n",
    "\n",
    "    policy = inference(get_distributions, init_pos, clip_env, clip_actor_critic, False)\n",
    "\n",
    "    def get_distributions(obs_seq, actions_tensor, clip_actor_critic):\n",
    "        dist = torch.distributions.Categorical(probs=torch.tensor([0.5, 0.25, 0.25], device=DEVICE))\n",
    "        return dist\n",
    "\n",
    "    rand_policy = inference(get_distributions, init_pos, clip_env, clip_actor_critic, False)\n",
    "\n",
    "    mean_policies = []\n",
    "    mean_rands = []\n",
    "    for i in range(10):\n",
    "        mean_policy = get_average(policy)\n",
    "        mean_rand = get_average(rand_policy)\n",
    "        mean_policies.append(mean_policy)\n",
    "        mean_rands.append(mean_rand)\n",
    "    \n",
    "    print(\"Policy: \" + str(np.mean(np.array(mean_policies))))\n",
    "    print(\"Rand: \" + str(np.mean(np.array(mean_rands))))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nav_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
