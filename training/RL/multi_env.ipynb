{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014209f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AI2-THOR WARNING] There has been an update to ProcTHOR-10K that must be used with AI2-THOR version 5.0+. To use the new version of ProcTHOR-10K, please update AI2-THOR to version 5.0+ by running:\n",
      "    pip install --upgrade ai2thor\n",
      "Alternatively, to downgrade to the old version of ProcTHOR-10K, run:\n",
      "   prior.load_dataset(\"procthor-10k\", revision=\"ab3cacd0fc17754d4c080a3fd50b18395fae8647\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train: 100%|██████████| 10000/10000 [00:00<00:00, 11485.66it/s]\n",
      "Loading val: 100%|██████████| 1000/1000 [00:00<00:00, 12006.26it/s]\n",
      "Loading test: 100%|██████████| 1000/1000 [00:00<00:00, 12101.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict(\n",
       "    train=Dataset(\n",
       "    dataset=procthor-dataset,\n",
       "    size=10000,\n",
       "    split=train\n",
       "),\n",
       "    val=Dataset(\n",
       "    dataset=procthor-dataset,\n",
       "    size=1000,\n",
       "    split=val\n",
       "),\n",
       "    test=Dataset(\n",
       "    dataset=procthor-dataset,\n",
       "    size=1000,\n",
       "    split=test\n",
       ")\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import prior\n",
    "\n",
    "dataset = prior.load_dataset(\"procthor-10k\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a00270de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def teleport(controller, target=None):\n",
    "    event = controller.step(\"GetReachablePositions\")\n",
    "    reachable_positions = event.metadata[\"actionReturn\"]\n",
    "    # Pick a random target\n",
    "    if target is None:\n",
    "        target = np.random.choice(reachable_positions)\n",
    "\n",
    "    event = controller.step(\n",
    "        action=\"TeleportFull\",\n",
    "        x=target[\"x\"],\n",
    "        y=target[\"y\"],\n",
    "        z=target[\"z\"],\n",
    "        rotation={\"x\": 0, \"y\": 0, \"z\": 0},\n",
    "        horizon=0,\n",
    "        standing=True\n",
    "    )\n",
    "\n",
    "    return event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cab0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl import PPO, CLIPNovelty, ClipEnv, ActorCritic\n",
    "from models import FrozenResNetEncoder, SlidingWindowTransformerActor, SlidingWindowTransformerCritic\n",
    "from cons import FEAT_DIM, NUM_ACTIONS\n",
    "\n",
    "ENTROPY_COEF = 0.05\n",
    "\n",
    "ppo = PPO(ENTROPY_COEF)\n",
    "encoder = FrozenResNetEncoder(project_to_out_dim=False)\n",
    "actor = SlidingWindowTransformerActor(FEAT_DIM, NUM_ACTIONS) # Not important\n",
    "critic = SlidingWindowTransformerCritic(FEAT_DIM) # Not important\n",
    "clip_novelty = CLIPNovelty()\n",
    "clip_env = ClipEnv(clip_novelty)\n",
    "clip_actor_critic = ActorCritic(encoder, actor, critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44be7b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/ai2thor/platform.py:154: UserWarning: could not connect to X Display: 5, Can't connect to display \":5\": b'Authorization required, but no authorization protocol specified\\n'\n",
      "  warnings.warn(\n",
      "/home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/ai2thor/platform.py:154: UserWarning: could not connect to X Display: 5, Can't connect to display \":5\": b'Authorization required, but no authorization protocol specified\\n'\n",
      "  warnings.warn(\n",
      "/home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/ai2thor/platform.py:154: UserWarning: could not connect to X Display: 5, Can't connect to display \":5\": b'Authorization required, but no authorization protocol specified\\n'\n",
      "  warnings.warn(\n",
      "/home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/ai2thor/platform.py:154: UserWarning: could not connect to X Display: 5, Can't connect to display \":5\": b'Authorization required, but no authorization protocol specified\\n'\n",
      "  warnings.warn(\n",
      "/home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/ai2thor/platform.py:154: UserWarning: could not connect to X Display: 5, Can't connect to display \":5\": b'Authorization required, but no authorization protocol specified\\n'\n",
      "  warnings.warn(\n",
      "/home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/ai2thor/platform.py:154: UserWarning: could not connect to X Display: 5, Can't connect to display \":5\": b'Authorization required, but no authorization protocol specified\\n'\n",
      "  warnings.warn(\n",
      "/home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/ai2thor/platform.py:154: UserWarning: could not connect to X Display: 5, Can't connect to display \":5\": b'Authorization required, but no authorization protocol specified\\n'\n",
      "  warnings.warn(\n",
      "/home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/ai2thor/platform.py:154: UserWarning: could not connect to X Display: 5, Can't connect to display \":5\": b'Authorization required, but no authorization protocol specified\\n'\n",
      "  warnings.warn(\n",
      "/home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/ai2thor/platform.py:154: UserWarning: could not connect to X Display: 5, Can't connect to display \":5\": b'Authorization required, but no authorization protocol specified\\n'\n",
      "  warnings.warn(\n",
      "/home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/ai2thor/platform.py:154: UserWarning: could not connect to X Display: 5, Can't connect to display \":5\": b'Authorization required, but no authorization protocol specified\\n'\n",
      "  warnings.warn(\n",
      "/home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/ai2thor/platform.py:154: UserWarning: could not connect to X Display: 5, Can't connect to display \":5\": b'Authorization required, but no authorization protocol specified\\n'\n",
      "  warnings.warn(\n",
      "/home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/ai2thor/platform.py:154: UserWarning: could not connect to X Display: 5, Can't connect to display \":5\": b'Authorization required, but no authorization protocol specified\\n'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 200x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 200x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 200x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 200x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 200x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 200x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 200x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 200x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 200x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 200x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 200x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 200x6400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rl import inference, teleport\n",
    "import torch\n",
    "from ai2thor.controller import Controller\n",
    "\n",
    "\n",
    "def get_distribution(ppo, obs_seq, actions_seq, actor_critic):\n",
    "    dist = torch.distributions.Categorical(probs=torch.tensor([0.5, 0.25, 0.25]))\n",
    "    return dist\n",
    "\n",
    "all_obs = []\n",
    "\n",
    "for i in range(12):\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Pick a different random environment from your dataset\n",
    "    # ------------------------------------------------------------\n",
    "    idx = torch.randint(0, len(dataset[\"train\"]), (1,)).item()\n",
    "    house = dataset[\"train\"][idx]\n",
    "\n",
    "    controller = Controller(scene=house, snapToGrid=False, rotateStepDegrees=30)\n",
    "\n",
    "    try:\n",
    "        # teleport agent inside this new house\n",
    "        event = teleport(controller)\n",
    "        init_pos = event.metadata[\"agent\"][\"position\"]\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Run inference in THIS environment\n",
    "        # ------------------------------------------------------------\n",
    "        obs = inference(\n",
    "            get_distribution=get_distribution,\n",
    "            controller=controller,\n",
    "            ppo=ppo,\n",
    "            init_position=init_pos,\n",
    "            env=clip_env,\n",
    "            actor_critic=clip_actor_critic,\n",
    "            plot=False,\n",
    "            n=32\n",
    "        )\n",
    "\n",
    "        all_obs.append(torch.stack(obs, dim=0))\n",
    "\n",
    "    finally:\n",
    "        # Always clean up controller\n",
    "        controller.stop()\n",
    "\n",
    "\n",
    "all_obs_tensor = torch.cat(all_obs, dim=0)\n",
    "embedding = encoder(all_obs_tensor.unsqueeze(0)).squeeze(0)\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "pca = PCA(n_components=FEAT_DIM)\n",
    "pca.fit(embedding.detach().cpu().numpy())\n",
    "W = pca.components_.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b251e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl import PPO, ActorCritic, Env, RolloutBuffer, ClipEnv, CLIPNovelty\n",
    "from models import LSTMActor, LSTMCritic, FrozenResNetEncoder, SlidingWindowTransformerActor, SlidingWindowTransformerCritic\n",
    "from cons import MINIBATCHES, EPISODE_STEPS, FEAT_DIM, NUM_ACTIONS, DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4897dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "from ai2thor.controller import Controller\n",
    "from rl import save_actor_critic, RolloutBuffer, MINIBATCHES, EPISODE_STEPS, DEVICE, teleport\n",
    "\n",
    "\n",
    "def train(\n",
    "    name: str,\n",
    "    ppo: PPO,\n",
    "    env_cls,\n",
    "    actor_critic: ActorCritic,\n",
    "    total_updates=10,\n",
    "    num_envs_per_minibatch=8\n",
    "):\n",
    "    # ------------------------------------------------------------\n",
    "    # W&B init\n",
    "    # ------------------------------------------------------------\n",
    "    run = wandb.init(\n",
    "        reinit=\"finish_previous\",\n",
    "        entity=\"viriyadhika1\",\n",
    "        project=\"cv-final-project\",\n",
    "        name=name,\n",
    "        config={},\n",
    "    )\n",
    "\n",
    "    rewards = []\n",
    "    episode_rewards = []\n",
    "\n",
    "    try:\n",
    "        for upd in range(total_updates):\n",
    "            buf = RolloutBuffer()\n",
    "\n",
    "            # ======================================================\n",
    "            # ============= MINI-BATCH LOOP ========================\n",
    "            # ======================================================\n",
    "            for mb in range(1):\n",
    "\n",
    "                # ======================================================\n",
    "                # Run num_envs_per_minibatch EPISODES sequentially\n",
    "                # ======================================================\n",
    "                for env_idx in range(num_envs_per_minibatch):\n",
    "\n",
    "                    # ----------------------------------------------------\n",
    "                    # Create ONE controller for this episode\n",
    "                    # ----------------------------------------------------\n",
    "                    idx = torch.randint(0, len(dataset[\"train\"]), (1,)).item()\n",
    "                    house = dataset[\"train\"][idx]\n",
    "\n",
    "                    controller = Controller(scene=house, snapToGrid=False, rotateStepDegrees=30, renderInstanceSegmentation=True)\n",
    "                    env = env_cls()\n",
    "\n",
    "                    try:\n",
    "                        # Start the episode\n",
    "                        event = teleport(controller)\n",
    "                        episode_seq = []\n",
    "                        actions_seq = []\n",
    "                        episode_reward = 0.0\n",
    "\n",
    "                        # ======================================================\n",
    "                        # ============= EPISODE LOOP ============================\n",
    "                        # ======================================================\n",
    "                        for t in range(1, EPISODE_STEPS + 1):\n",
    "                            with torch.no_grad():\n",
    "                                # Get observation and encode\n",
    "                                obs_t = ppo.obs_from_event(event)\n",
    "                                obs_enc = actor_critic.actor_critic_encoder(\n",
    "                                    obs_t.unsqueeze(0).unsqueeze(0)\n",
    "                                ).squeeze(0).squeeze(0)\n",
    "    \n",
    "                                # Build sequence tensor\n",
    "                                obs_seq = torch.stack(\n",
    "                                    episode_seq + [obs_enc], dim=0\n",
    "                                ).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "                            # Random init action (new version style)\n",
    "                            if len(actions_seq) == 0:\n",
    "                                a0 = torch.randint(0, NUM_ACTIONS, (1, 1)).item()\n",
    "                                actions_seq.append(a0)\n",
    "\n",
    "                            actions_tensor = torch.tensor(\n",
    "                                actions_seq, dtype=torch.long\n",
    "                            ).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "                            # Policy forward\n",
    "                            logits, value = ppo.act_and_value(obs_seq, actions_tensor, actor_critic)\n",
    "                            dist = torch.distributions.Categorical(logits=logits)\n",
    "                            action_idx = dist.sample().item()\n",
    "                            logp = dist.log_prob(torch.tensor(action_idx, device=logits.device)).item()\n",
    "\n",
    "                            # Environment step\n",
    "                            event, reward = env.step_env(controller, action_idx)\n",
    "                            done = (t == EPISODE_STEPS)\n",
    "\n",
    "                            # Store transition\n",
    "                            buf.add(obs_enc, action_idx, logp, reward, value, done)\n",
    "\n",
    "                            episode_seq.append(obs_enc)\n",
    "                            actions_seq.append(action_idx)\n",
    "\n",
    "                            rewards.append(reward)\n",
    "                            episode_reward += reward / EPISODE_STEPS\n",
    "\n",
    "                            wandb.log({\"reward\": reward})\n",
    "\n",
    "                            # Episode ended\n",
    "                            if done:\n",
    "                                env.reset()\n",
    "                                if np.random.rand() > 0.5:\n",
    "                                    event = teleport(controller)\n",
    "\n",
    "                        wandb.log({\"episode_reward\": episode_reward})\n",
    "                        episode_rewards.append(episode_reward)\n",
    "\n",
    "                    finally:\n",
    "                        controller.stop()\n",
    "\n",
    "            # ======================================================\n",
    "            # PPO UPDATE\n",
    "            # ======================================================\n",
    "            ppo.ppo_update(buf, actor_critic)\n",
    "\n",
    "            # Save model periodically\n",
    "            if (upd + 1) % 10 == 0:\n",
    "                save_actor_critic(actor_critic, f\"data/{name}_{upd}.pt\")\n",
    "\n",
    "            # Save latest\n",
    "            save_actor_critic(actor_critic, f\"data/{name}.pt\")\n",
    "\n",
    "            print(f\"Update {upd+1}/{total_updates} — steps: {len(buf)}\")\n",
    "\n",
    "    finally:\n",
    "        run.finish()\n",
    "\n",
    "    return buf, rewards, episode_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cac7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl import PPO, SegmentationNovelty, ClipEnv, ActorCritic\n",
    "from models import FrozenResNetPCAEncoder, SlidingWindowTransformerActor, SlidingWindowTransformerCritic\n",
    "from cons import FEAT_DIM, NUM_ACTIONS, DEVICE\n",
    "\n",
    "ENTROPY_COEF = 0.05\n",
    "\n",
    "ppo = PPO(ENTROPY_COEF)\n",
    "encoder = FrozenResNetPCAEncoder(FEAT_DIM, torch.from_numpy(W), device=DEVICE)\n",
    "actor = SlidingWindowTransformerActor(FEAT_DIM, NUM_ACTIONS)\n",
    "critic = SlidingWindowTransformerCritic(FEAT_DIM)\n",
    "def env_cls():\n",
    "    clip_novelty = SegmentationNovelty()\n",
    "    clip_env = ClipEnv(clip_novelty) \n",
    "    return clip_env\n",
    "\n",
    "clip_actor_critic = ActorCritic(encoder, actor, critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0096961c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mviriyadhika-putra\u001b[0m (\u001b[33mviriyadhika1\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/juyuanli/Viri/NavAssistant/RL/wandb/run-20251129_230350-v4lk2cjo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/viriyadhika1/cv-final-project/runs/v4lk2cjo' target=\"_blank\">multi_env.pt</a></strong> to <a href='https://wandb.ai/viriyadhika1/cv-final-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/viriyadhika1/cv-final-project' target=\"_blank\">https://wandb.ai/viriyadhika1/cv-final-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/viriyadhika1/cv-final-project/runs/v4lk2cjo' target=\"_blank\">https://wandb.ai/viriyadhika1/cv-final-project/runs/v4lk2cjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/ai2thor/platform.py:154: UserWarning: could not connect to X Display: 5, Can't connect to display \":5\": b'Authorization required, but no authorization protocol specified\\n'\n",
      "  warnings.warn(\n",
      "/home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/ai2thor/platform.py:154: UserWarning: could not connect to X Display: 5, Can't connect to display \":5\": b'Authorization required, but no authorization protocol specified\\n'\n",
      "  warnings.warn(\n",
      "/home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/ai2thor/platform.py:154: UserWarning: could not connect to X Display: 5, Can't connect to display \":5\": b'Authorization required, but no authorization protocol specified\\n'\n",
      "  warnings.warn(\n",
      "/home/juyuanli/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/ai2thor/platform.py:154: UserWarning: could not connect to X Display: 5, Can't connect to display \":5\": b'Authorization required, but no authorization protocol specified\\n'\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode_reward</td><td>▃█▁▂</td></tr><tr><td>reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode_reward</td><td>0.12495</td></tr><tr><td>reward</td><td>0.09542</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">multi_env.pt</strong> at: <a href='https://wandb.ai/viriyadhika1/cv-final-project/runs/v4lk2cjo' target=\"_blank\">https://wandb.ai/viriyadhika1/cv-final-project/runs/v4lk2cjo</a><br> View project at: <a href='https://wandb.ai/viriyadhika1/cv-final-project' target=\"_blank\">https://wandb.ai/viriyadhika1/cv-final-project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251129_230350-v4lk2cjo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 31.37 GiB of which 77.50 MiB is free. Process 173924 has 6.36 GiB memory in use. Process 3169638 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 3.87 GiB memory in use. Of the allocated memory 3.22 GiB is allocated by PyTorch, and 40.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m buf, rewards, episode_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmulti_env.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mppo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_actor_critic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_envs_per_minibatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 121\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(name, ppo, env_cls, actor_critic, total_updates, num_envs_per_minibatch)\u001b[0m\n\u001b[1;32m    116\u001b[0m             controller\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# ======================================================\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# PPO UPDATE\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# ======================================================\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mppo_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_critic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Save model periodically\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (upd \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Viri/NavAssistant/RL/rl.py:439\u001b[0m, in \u001b[0;36mPPO.ppo_update\u001b[0;34m(self, buffer, actor_critic)\u001b[0m\n\u001b[1;32m    435\u001b[0m advantages \u001b[38;5;241m=\u001b[39m (advantages \u001b[38;5;241m-\u001b[39m advantages\u001b[38;5;241m.\u001b[39mmean()) \u001b[38;5;241m/\u001b[39m (advantages\u001b[38;5;241m.\u001b[39mstd() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(TRAIN_EPOCHS):\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;66;03m# Forward through actor + critic\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m     new_logp, entropy, value_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_critic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m TRAIN_EPOCHS \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/Viri/NavAssistant/RL/rl.py:393\u001b[0m, in \u001b[0;36mPPO.evaluate_batch\u001b[0;34m(self, feats, actions, actor_critic)\u001b[0m\n\u001b[1;32m    390\u001b[0m actions_seq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39mzeros_like(actions[:, :\u001b[38;5;241m1\u001b[39m]), actions[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# prev actions\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# ---- [2] actor forward ----\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mactor_critic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B*S, num_actions)\u001b[39;00m\n\u001b[1;32m    394\u001b[0m dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(logits\u001b[38;5;241m=\u001b[39mlogits)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# ---- [3] log-probs, entropy, and critic ----\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Viri/NavAssistant/RL/models.py:311\u001b[0m, in \u001b[0;36mSlidingWindowTransformerActor.forward\u001b[0;34m(self, X, actions_seq, mask)\u001b[0m\n\u001b[1;32m    308\u001b[0m flat \u001b[38;5;241m=\u001b[39m flat \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# ---- 3. Transformer ----\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformer_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat\u001b[49m\u001b[43m)\u001b[49m           \u001b[38;5;66;03m# (B*S, W, D)\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# ---- 4. Use last token only ----\u001b[39;00m\n\u001b[1;32m    314\u001b[0m last_token \u001b[38;5;241m=\u001b[39m z[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]       \u001b[38;5;66;03m# (B*S, D)\u001b[39;00m\n",
      "File \u001b[0;32m~/Viri/NavAssistant/RL/models.py:276\u001b[0m, in \u001b[0;36mSlidingWindowTransformerActor._transformer_chunked\u001b[0;34m(self, x, chunk_size)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, N, chunk_size):\n\u001b[1;32m    275\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m x[i:i\u001b[38;5;241m+\u001b[39mchunk_size]   \u001b[38;5;66;03m# (chunk, W, D)\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m        \u001b[38;5;66;03m# transformer on mini-batch\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/torch/nn/modules/transformer.py:524\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    521\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 524\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    532\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/torch/nn/modules/transformer.py:937\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[1;32m    934\u001b[0m         x\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m    936\u001b[0m     )\n\u001b[0;32m--> 937\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/torch/nn/modules/transformer.py:962\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 962\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nav_assistant/lib/python3.11/site-packages/torch/nn/functional.py:1422\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1422\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1423\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 31.37 GiB of which 77.50 MiB is free. Process 173924 has 6.36 GiB memory in use. Process 3169638 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 3.87 GiB memory in use. Of the allocated memory 3.22 GiB is allocated by PyTorch, and 40.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "buf, rewards, episode_rewards = train(\n",
    "    \"multi_env.pt\",\n",
    "    ppo,\n",
    "    env_cls,\n",
    "    clip_actor_critic,\n",
    "    total_updates=5,\n",
    "    num_envs_per_minibatch=4\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nav_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
